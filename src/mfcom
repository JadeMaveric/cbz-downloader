#!/bin/bash

### mfcom Usage:help
#
# MangaFox Compiler
#
# Get all pages from a MangaFox commic to a local directory
#
# USAGE
#
# 	mfcom URL [ START [[+]END] ]
#
# Specify the URL of the main manga page
#
# Optionally specify the start chapter
#
# Optionally specify the end chapter, or a "+" followed by the number of chapters to download
#
# CONFIGURATION
#
# Set the following environment variables to configure mfcom:
#
# MFCOM_UAGENT="<user agent string>"
# 	set a custom user agent string instead of the default (Mozilla 52)
#
# MFCOM_MAKE_ARCHIVE={pdf | cbz}
# 	tries to compile to PDF or CBZ comic archive
#
# 	PDF requires img2pdf (apt install img2pdf)
# 	CBZ requires zip     (apt install zip)
#
# MFCOM_WAIT=<int>
# 	time in seconds to wait between operations ; default is 1
# 	mangafox seems to throttle access from IPs that download too much too fast
# 	set to 0 for no delay
#
###/doc

set -euo pipefail

SURLFILE="source.url"
LASTINDEXFILE="last-tried.txt"

#%include runmain.sh
#%include out.sh autohelp.sh varify.sh
#%include comicarchive

wgetdl() {
	local downloadfile="$1"; shift
	if [[ -z "$*" ]]; then
		out:warn "Error downloading $downloadfile"
		return 404
	fi

	wget --header="User-Agent: $MFCOM_UAGENT" --header="Accept: text/html, image/*" --quiet -O "$downloadfile" --no-check-certificate "$@" || :
	do_unzip "$downloadfile"
}

get_mainList() {
	wgetdl "$F_MAIN" "$U_MAINPAGE"
}

extract_links() {
	local sourcefile="$1"; shift
	local targetfile="$1"; shift

	grep "$U_MAINPAGE_NOSCHEME" "$sourcefile" | \
		grep -Po 'a href="'"$U_MAINPAGE_NOSCHEME"'.*?"' | \
		grep -Po "$U_MAINPAGE_NOSCHEME[^\"]+" | \
		sed -r 's|^|http:|' | tac \
		> "$targetfile"
}

unzero() {
        local unzeroed="$(echo "$1"|sed -r s/^0+//)"
        if [[ -z "$unzeroed" ]]; then
                echo "0"
        else
                echo "$unzeroed"
        fi
}

is_bigger_or_eq() {
	[[ ! $(dc -e "$1 $2 - p") =~ - ]] || return 1
}

is_smaller() {
	! is_bigger_or_eq "$1" "$2"
}

do_unzip() {
	if (file "$1"|grep gzip -q); then
		mv "$1" "$1.gz"
		gunzip "$1.gz"
	fi
}

set_image_extension() {
	local imagefile="$1"; shift
	local imagetype="$(file "$imagefile"|egrep -oi 'jpeg|jpg|png|gif' | tr '[:upper:]' '[:lower:]')"
	local extension=bin

	out:debug "Renaming $imagefile"

	case "$imagetype" in
		jpeg|jpg)
			extension=jpg ;;
		png|gif)
			extension="$imagetype" ;;
		*)
			out:warn "Could not determine extension for $imagefile"
			return ;;
	esac

	mv "$imagefile" "$imagefile.$extension"

	lastextension="$extension"
}

download_chapter_page() {
	local pageurl="$1"; shift
	local targetfile="$1"; shift

	out:info "Download [$pageurl] to [$targetfile]"

	wgetdl "$targetfile" "$pageurl"
	[[ "$?" = 404 ]] && out:fail 404 "Page not found"

	local imgurl="$(grep '<img' "$targetfile"|grep 'id="image"'|grep -oP 'src="[^"]+')"

	out:debug "Cutting $imgurl"

	imgurl="${imgurl:5}"

	out:debug "$imgurl : $targetfile"
	wgetdl "$targetfile" "$imgurl"
	[[ "$?" = 404 ]] && out:fail 404 "Image not found"
	set_image_extension "$targetfile"
}

padnum() {
	printf "%03d" "$1"
}

download_chapter() {
	local chapter_url="$1"; shift
	local base_url="$(dirname "$chapter_url")"
	local targetdir="$1"; shift
	local chapnum="$1"; shift

	targetdir="$targetdir/${D_TARGET}-${chapnum}"

	local dlfile="$targetdir/fpage.html"
	local chapdir="$(basename "$targetdir")"

	mkdir -p "$targetdir"

	# Register the last attempt now, in case fail before end
	echo "$chapnum" > "$F_LASTDONE"

	wgetdl "$dlfile" "$chapter_url"

	local pagecount="$(grep -oP 'of\s*\d+' "$dlfile"|sort|uniq|cut -d' ' -f2)"

	local i=0
	while [[ "$i" -lt "$pagecount" ]]; do
		i=$((i+1))
		download_chapter_page "$base_url/$i.html" "$targetdir/page_$(padnum "$i")"
		sleep "$MFCOM_WAIT"
	done

	[[ "$i" -gt 0 ]] || {
		out:warn "Nothing downloaded !"
		return
	}

	local archivename="$(varify:fil "$chapdir")"
	if [[ "${MFCOM_MAKE_ARCHIVE:-}" = pdf ]]; then
		make_archive:pdf "$targetdir"
	elif [[ "${MFCOM_MAKE_ARCHIVE:-}" = cbz ]]; then
		make_archive:cbz "$targetdir"
	fi
}

download_all_chapters() {
	local chapnum=0
	while read LINKLINE; do
		chappat="/c([0-9.]+)/?"
		[[ "$LINKLINE" =~ $chappat ]]
		chapnum="${BASH_REMATCH[1]}"
		if [[ -z "$chapnum" ]]; then
			out:warn "Could not extract chapter number from $LINKLINE"
			continue
		fi

		if [[ -n  "$STARTINDEX" ]]; then
			# chapter number < desired start index
			if is_smaller "$chapnum" "$STARTINDEX"; then continue; fi

			if [[ -n "$ENDINDEX" ]]; then
				# chapter number >= desired end index
				if is_bigger_or_eq "$chapnum" "$ENDINDEX" ; then break; fi
			fi
		fi
		
		download_chapter "$LINKLINE" "$D_TARGET" "${chapnum}"

	done < "$F_LINKS"
}

numcheck() {
	local tocheck="$1"; shift

	if [[ ! "$tocheck" =~ ^[0-9]*$ ]]; then
		out:fail "Invalid number: $*"
	fi
}

get_main_url() {
	U_MAINPAGE="$1"; shift

	if [[ -d "$U_MAINPAGE" ]] && [[ -f "$U_MAINPAGE/$SURLFILE" ]]; then
		U_MAINPAGE="$(cat "$U_MAINPAGE/$SURLFILE")"

	elif [[ ! "$U_MAINPAGE" =~ ^https?:// ]]; then
		out:fail "Invalid page URL $U_MAINPAGE"
	fi
}

set_indices() {
	STARTINDEX="${1:-}"
	ENDINDEX="${2:-}"

	[[ -n "$STARTINDEX" ]] || return 0

	numcheck "$STARTINDEX" start index

	[[ -n "$ENDINDEX" ]] || return 0

	if [[ "$ENDINDEX" =~ ^\+ ]]; then
		ENDINDEX="${ENDINDEX#+}"
		numcheck "$ENDINDEX" end index length
		ENDINDEX=$((STARTINDEX + ENDINDEX))
	else
		numcheck "$ENDINDEX" end index
		is_smaller "$STARTINDEX" "$ENDINDEX" || out:fail "Start index must be smaller than end index"
	fi
}

setup_configuration() {
	# Mangafox has started generating urls without schemes
	U_MAINPAGE_NOSCHEME="$(echo "$U_MAINPAGE"|sed -r 's|https?://|//|')"

	set_indices "${1:-}" "${2:-}"

	D_TARGET="$(varify:fil "${U_MAINPAGE##*/manga/}")"
	D_TARGET="${D_TARGET%_}"
	F_MAIN="$D_TARGET/frontpage.html"
	F_LINKS="$D_TARGET/links.txt"
	F_SOURCEURL="$D_TARGET/$SURLFILE"
	F_LASTDONE="$D_TARGET/$LASTINDEXFILE"

	mkdir -p "$D_TARGET"

	[[ -f "$F_SOURCEURL" ]] || echo "$U_MAINPAGE" > "$F_SOURCEURL"
	[[ -z "$STARTINDEX" ]] && [[ -f "$F_LASTDONE" ]] && STARTINDEX="$(cat "$F_LASTDONE")" || :
}

announce_setup() {
	echo "MFCOM_WAIT=$MFCOM_WAIT"
	echo "MFCOM_MAKE_ARCHIVE=${MFCOM_MAKE_ARCHIVE:-}"
	echo "Downloading from: $U_MAINPAGE"
	echo "Downloading to:   $D_TARGET"
	echo "Starting at: $STARTINDEX"
	echo "Ending at:   $ENDINDEX"
	sleep 2
}

main() {
	if [[ -z "$*" ]]; then
		autohelp:print
		exit
	fi

	: ${MFCOM_UAGENT="Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0"}

	: ${MFCOM_WAIT=2}

	get_main_url "$1"; shift
	setup_configuration "${1:-}" "${2:-}"
	shift 2 || :

	announce_setup

	get_mainList
	extract_links "$F_MAIN" "$F_LINKS"
	download_all_chapters
}

runmain mfcom main "$@"

