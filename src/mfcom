#!/bin/bash

### mfcom Usage:help
#
# MangaFox Compiler
#
# Get all pages from a MangaFox commic to a local directory
#
# USAGE
#
# 	mfcom URL [ START [END] ]
#
# Specify the URL of the main manga page
#
# Optionally specify the start chapter
#
# Optionally specify the end chapter
#
# CONFIGURATION
#
# Set the following environment variables to configure mfcom:
#
# MFCOM_UAGENT="<user agent string>"
# 	set a custom user agent string instead of the default (Mozilla 52)
#
# MFCOM_MAKE_PDF=true
# 	tries to compile to PDF
# 	requires img2pdf, otherwise uses the default action
#
# 	The default action is to create a CBZ file (which is in fact a .tar.gz file)
#
# MFCOM_WAIT=<int>
# 	time in seconds to wait between operations ; default is 1
# 	mangafox seems to throttle access from IPs that download too much too fast
#
###/doc

set -euo pipefail

#%include version.sh notest.sh
#%include bashout.sh autohelp.sh varify.sh getbin.sh

function wgetdl {
	local downloadfile="$1"; shift
	wget --header="$MFCOM_UAGENT" --header="Accept: text/html, image/*" --quiet -O "$downloadfile" --no-check-certificate "$@" || :
	do_unzip "$downloadfile"
}

function get_mainList {
	wgetdl "$F_MAIN" "$U_MAINPAGE"
}

function extract_links {
	local sourcefile="$1"; shift
	local targetfile="$1"; shift

	grep "$U_MAINPAGE_NOSCHEME" "$sourcefile" | \
		grep -Po 'a href="'"$U_MAINPAGE_NOSCHEME"'.*?"' | \
		grep -Po "$U_MAINPAGE_NOSCHEME[^\"]+" | \
		sed -r 's|^|http:|' | tac \
		> "$targetfile"
}

function do_unzip {
	if (file "$1"|grep gzip -q); then
		mv "$1" "$1.gz"
		gunzip "$1.gz"
	fi
}

function padnum {
	printf %03d "$1"
}

function set_image_extension {
	local imagefile="$1"; shift
	local imagetype="$(file "$imagefile"|egrep -oi 'jpeg|jpg|png|gif' | tr '[:upper:]' '[:lower:]')"
	local extension=bin

	infoe "Renaming $imagefile"

	case "$imagetype" in
		jpeg|jpg)
			extension=jpg ;;
		png)
			extension=png ;;
		gif)
			extension=gif ;;
		*)
			warne "Could not determine extension for $1"
			return ;;
	esac

	mv "$imagefile" "$imagefile.$extension"

	lastextension="$extension"
}

function download_chapter_page {
	local pageurl="$1"; shift
	local targetfile="$1"; shift

	wgetdl "$targetfile" "$pageurl"

	local imgurl="$(grep '<img' "$targetfile"|grep 'id="image"'|grep -oP 'src="[^"]+')"

	debuge "Cutting $imgurl"

	imgurl="${imgurl:5}"

	debuge "$imgurl : $targetfile"
	wgetdl "$targetfile" "$imgurl"
	set_image_extension "$targetfile"
}

function download_chapter {
	local chapter_url="$1"; shift
	local base_url="$(dirname "$chapter_url")"
	local targetdir="$1"; shift
	local dlfile="$targetdir/fpage.html"
	local chapdir="$(basename "$targetdir")"

	mkdir -p "$targetdir"

	wgetdl "$dlfile" "$chapter_url"

	local pagecount="$(grep -oP 'of\s*\d+' "$dlfile"|sort|uniq|cut -d' ' -f2)"

	local i=0
	while [[ "$i" -lt "$pagecount" ]]; do
		i=$((i+1))
		download_chapter_page "$base_url/$i.html" "$targetdir/page_$(padnum "$i")"
		sleep "$MFCOM_WAIT"
	done

	local archivename="$(varify_fil "$chapdir")"
	if hasbin img2pdf && [[ "${MFCOM_MAKE_PDF:-}" = true ]]; then
		(
			cd "$targetdir/.."
			img2pdf -o "$archivename.pdf" "$chapdir"/*."$lastextension" && {
				rm -r "$chapdir"
			}
		)
	else
		(
			# Comic book archives need a special comic book reader
			#  but they are recognized by eBook readers, and can be unpacked
			#  since they're literally a TAR.GZ file
			#  and displayed in name order ...!
			cd "$targetdir/../$chapdir"
			tar czf "$archivename.cbz" *."$lastextension" && {
				mv "$archivename.cbz" ../
				cd ../
				rm -r "$chapdir"
			}
		)
	fi
}

function download_all_chapters {
	local i=0
	while read LINKLINE; do
		i=$((i+1))
		chappat="/c[0-9]*$i/?"

		if [[ -n  "$STARTINDEX" ]]; then
			#if [[ ! "$LINKLINE" =~ $chappat ]]; then continue; fi
			if [[ "$i" -lt "$STARTINDEX" ]]; then continue; fi

			if [[ -n "$ENDINDEX" ]]; then
				if [[ ! "$i" -le "$ENDINDEX" ]]; then break; fi
			fi
		fi
		
		download_chapter "$LINKLINE" "$D_TARGET/chapter_$(padnum "$i")"

	done < "$F_LINKS"
}

function numcheck {
	local tocheck="$1"; shift

	if [[ ! "$tocheck" =~ ^[0-9]*$ ]]; then
		faile Invalid "$*"
	fi
}

function main {
	if [[ -z "$*" ]]; then
		printhelp
		exit
	fi

	: ${MFCOM_UAGENT="User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0"}

	: ${MFCOM_WAIT=2}
	echo "MFCOM_WAIT=$MFCOM_WAIT"

	U_MAINPAGE="$1"; shift
	# Mangafox has started generating urls without schemes
	U_MAINPAGE_NOSCHEME="$(echo "$U_MAINPAGE"|sed -r 's|https?://|//|')"

	STARTINDEX="${1:-}"
	ENDINDEX="${2:-}"

	numcheck "$STARTINDEX" start index
	numcheck "$ENDINDEX" end index

	H_MANGA="$(echo "$U_MAINPAGE"|md5sum)"
	H_MANGA="${H_MANGA:0:6}"
	D_TARGET="$(varify_fil "$H_MANGA-$U_MAINPAGE")"
	F_MAIN="$D_TARGET/frontpage.html"
	F_LINKS="$D_TARGET/links.txt"

	mkdir -p "$D_TARGET"

	get_mainList
	extract_links "$F_MAIN" "$F_LINKS"
	download_all_chapters
}

notest main "$@"

